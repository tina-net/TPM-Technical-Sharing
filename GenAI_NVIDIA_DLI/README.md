# ğŸ¤– NVIDIA DLI: Generative AI Explained

This note covers the fundamentals of LLMs and generative AI, including Transformer architecture, inference workflow, and prompt engineering. Diagrams included.

æœ¬ç­†è¨˜ä»¥ä¸­è‹±æ–‡æ•´ç† LLM èˆ‡ Transformer æ¶æ§‹åŸç†ï¼Œæ­é…æ¨è«–æµç¨‹åœ–ã€Prompt è¨­è¨ˆç¤ºæ„ï¼Œå¼·åŒ– TPM å°ç”Ÿæˆå¼ AI æ¶æ§‹çš„ç†è§£èˆ‡æºé€šèƒ½åŠ›ã€‚

---

## ğŸ” Key Topics
- ğŸ”¸ Transformer æ¶æ§‹èˆ‡ Self-Attention
- ğŸ”¸ LLM æ¨è«–æµç¨‹ï¼ˆChatGPT é¡æ¨¡å‹ï¼‰
- ğŸ”¸ Prompt Engineering æ¦‚å¿µèˆ‡æ‡‰ç”¨æƒ…å¢ƒ

- # ğŸ¤– NVIDIA DLI: Generative AI Explained | ä¸­è‹±ç­†è¨˜æ•´ç†

This is a bilingual summary of the NVIDIA Deep Learning Institute (DLI) course "Generative AI Explained." It introduces the architecture of generative models, especially transformers and large language models (LLMs), with visual workflows and core concepts suitable for TPMs.

---

## ğŸ“˜ Course Overview èª²ç¨‹æ¦‚è¦½

- English: This course provides a high-level explanation of how generative AI models like ChatGPT work, covering transformer architecture, tokenization, and inference logic.
- ä¸­æ–‡ï¼šæœ¬èª²ç¨‹ä»¥é«˜éšè§’åº¦è§£é‡‹ç”Ÿæˆå¼ AI çš„é‹ä½œæ–¹å¼ï¼Œé‡é»ä»‹ç´¹ Transformer æ¶æ§‹ã€Token åŒ–æ©Ÿåˆ¶èˆ‡æ¨è«–é‚è¼¯ã€‚

---

## ğŸ” Key Concepts æ ¸å¿ƒæ¦‚å¿µ

### 1. What is Generative AI? | ä»€éº¼æ˜¯ç”Ÿæˆå¼ AIï¼Ÿ
- Models that create content (text, image, audio) instead of just classifying or predicting.
- ç”Ÿæˆå¼ AI æ¨¡å‹å¯ç”¢å‡ºæ–‡æœ¬ã€åœ–ç‰‡ã€éŸ³è¨Šç­‰ï¼Œä¸åƒ…é™æ–¼åˆ†é¡æˆ–é æ¸¬ã€‚

### 2. Transformer Architecture | Transformer æ¶æ§‹è§£æ
- Key elements: Self-Attention, Positional Encoding, Multi-Head Attention
- æ ¸å¿ƒçµ„ä»¶ï¼šè‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶ã€ä½ç½®ç·¨ç¢¼ã€å¤šé ­æ³¨æ„åŠ›
- TPM Insightï¼šæœ‰åŠ©æ–¼ç†è§£ LLM çš„é‹ä½œèˆ‡éƒ¨ç½²è³‡æºéœ€æ±‚ã€‚

### 3. Large Language Models (LLMs) | å¤§å‹èªè¨€æ¨¡å‹
- Trained on massive datasets using encoder-decoder or decoder-only structures
- åŸºæ–¼å·¨é‡èªæ–™è¨“ç·´ï¼Œå¯æ¡ç”¨ Encoder-Decoder æˆ– Decoder-only çµæ§‹
- ç¤ºä¾‹ï¼šGPTã€LLaMAã€Gemini

### 4. Prompt Engineering | æç¤ºè¨­è¨ˆ
- Crafting inputs to steer model output effectively
- ç²¾æº–è¨­è¨ˆ Prompt å¯å¼•å°æ¨¡å‹ç”¢å‡ºæ›´ç¬¦åˆéœ€æ±‚çš„çµæœ
- TPM Insightï¼šå½±éŸ¿ç”¢å“æ¨è«–éšæ®µçš„ UX èˆ‡å¯æ§æ€§

### 5. Inference Pipeline | æ¨è«–æµç¨‹ç°¡ä»‹
- Tokenize input â†’ Run through Transformer layers â†’ Generate tokens â†’ Detokenize output
- è¼¸å…¥æ–‡å­— Token åŒ– â†’ ç¶“ Transformer å±¤é‹ç®— â†’ ç”¢ç”Ÿä¸‹å€‹ Token â†’ è§£ç¢¼é‚„åŸç‚ºæ–‡å­—

---

## âœ¨ TPM Perspective | å°ˆæ¡ˆç¶“ç†çš„è¦–è§’

- Understand model size and compute demand when scoping infrastructure  
  æ§åˆ¶ LLM éƒ¨ç½²çš„ GPU è³‡æºé ç®—  
- Collaborate with R&D and infra teams to integrate inference endpoints  
  èˆ‡è»Ÿé«”ã€åŸºç¤è¨­æ–½å°ˆæ¡ˆåˆä½œ  
- Plan for prompt quality testing & edge case validation  
  å‰ç«¯ä½¿ç”¨è€… prompt æ¸¬è©¦èˆ‡å•é¡Œè·é›¢è¨ºæ–·

---

## ğŸ“Œ Summary

This course equips TPMs with foundational understanding of how generative models function internally, helping them better plan infrastructure, explain capabilities to stakeholders, and improve coordination in deployment projects.

æœ¬èª²ç¨‹å¯å¹«åŠ© TPM å»ºç«‹ç”Ÿæˆå¼æ¨¡å‹çš„åŸºæœ¬ç†è§£ï¼Œé€²è€Œåœ¨è¦åŠƒè³‡æºã€è·¨éƒ¨é–€æºé€šèˆ‡æ¨è«–éƒ¨ç½²ä¸­ç™¼æ®æ›´å¤§å½±éŸ¿åŠ›ã€‚

---

